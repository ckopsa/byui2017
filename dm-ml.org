#+TITLE: Data Mining and Machine Learning
#+DATE: <2017-01-06 Fri>
#+AUTHOR: Colton Kopsa
#+EMAIL: Aghbac@Aghbac.local
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not "LOGBOOK") date:t
#+OPTIONS: e:t email:nil f:t inline:t num:t p:nil pri:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t toc:t todo:t |:t
#+CREATOR: Emacs 25.1.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* Week 01 - Introduction to Machine Learning
* Week 02 - k-Nearest Neighbors
** 01 PROVE : ASSIGNMENT - EXPERIMENT SHELL & HARDCODED CLASSIFIER
*** Submission
    When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

    Name: Colton Kopsa

    1. Please provide the URL of your public GitHub repository.

       https://github.com/coljamkop/k-nearest-neighbors

    For questions 2-6, please type "True" or "False" in front of the question number.

    2. My experiment shell can correctly load the Iris data-set:
       True
    3. My experiment shell randomizes the order of the instances (making sure to keep instances lined up with their appropriate targets) it and splits the data into a training set (70%) and a test set (30%)?
       True
    4. I have created a HardCoded classifier class with two methods: train and predict. The train method accepts training data (including targets). The predict method returns a prediction or classification for each instance it receives.
       True 
    5. The Experime  
    print calc_entropy(2/3.0, 1/3.0)
    nt Shell, processes the data, passes the training data to the classifier's€™s train method, the test data to the predict method, and then compares the predicted values against the correct answers, to produce an overall accuracy (on the test set).
    True
    6. I have run the HardCodeWhen your assignment is complete, please answer the questions in this text file and upload it to I-Learn.


1. Please provide a link to your classifier in your public GitHub repo.
   https://github.com/coljamkop/Decision-Tree
2. Briefly describe your overall approach to the task and highlight the most
   difficult part of this assignment.

   My overall approach was to break the project up into little pieces and
   then combine them all in the en1d. I started with an entropy function, then
   added the functionality to handle multiple arguments, then made a function
   that would find the entropy for a given feature, then for a given column,
   then find the column with the least entropy, and then use all of those
   functions to help build the decision tree.

   The hardest part by far was conceptualizing how the decision tree was
   going to be put together. I kept on thinking of the data structure that I
   would create in order to handle this idea, but then I realized that I
   didn't need that, I just need a dictionary. When I was able to realize how
   my dreams could be accomplished with a dictionary, things started falling
   together rather quickly.

3. Briefly describe how you handled numeric data.

   Handling numeric data was pretty simple, I would nominalize it by creating
   bins to organize it by. Once you have the bin ranges for the data to be
   categorized then the /pandas/ library took care of the rest.

4. Briefly describe your you handled missing data.

   If data was missing it would just move to a known good node in the tree
   and keep going from there. Definitely not the most elegant solution, but
   it worked, and my accuracy was average above 90%. What the value defaults
   to if their is no data is definitely something that could be fine-tuned in
   the future.

5. Describe your results for the Iris data set. (e.g., What was the size of the tree? How did your implementation compare to existing implementations? How did your decision tree compare to your kNN classifier)

   In some cases, I was able to achieve 100%, and even with only using 10% of
   my data to train on, I would consistently get above 90%. So, that being
   said, the decision tree was more accurate than the kNN. 25 nodes excluding
   leaf nodes.

6. Include a textual representation of the tree your algorithm produced for the iris dataset.

   IF petal_width == large AND
       IF sepal_length == medium AND
   sepal_width == very_small
      THEN Iris-virginica
   sepal_width == small
      THEN Iris-virginica
   IF sepal_width == medium AND
       petal_length == medium
      THEN Iris-versicolor
   petal_length == large
      THEN Iris-virginica
   sepal_length == very-large
      THEN Iris-virginica
   sepal_length == small
      THEN Iris-virginica
   IF sepal_length == large AND
       sepal_width == very_small
      THEN Iris-virginica
   sepal_width == small
      THEN Iris-versicolor
   petal_width == very-large
      THEN Iris-virginica
   petal_width == very_small
      THEN Iris-setosa
   IF petal_width == medium AND
       petal_length == small
      THEN Iris-versicolor
   IF petal_length == large AND
       IF sepal_length == medium AND
   sepal_width == small
      THEN Iris-virginica
   sepal_width == very_very_small
      THEN Iris-virginica
   sepal_length == large
      THEN Iris-virginica
   petal_length == medium
      THEN Iris-versicolor
   petal_width == small
      THEN Iris-versicolor
   petal_width == very_very_small
      THEN Iris-setosa

7. Describe your results for the Lenses data set. (e.g., What was the size of the tree? How did your implementation compare to existing implementations?)

   The lenses data set performed fairly well considering the sparse amount of
   data. Giving it 80% of the data to train on and 20% to test on yielded
   great results, averaging around 90%, but fluctuating from 80% to 100%. I
   couldn't figure out how to get Weka to work with the lense data. It would
   give me poor numbers consistently. I assume it would compare well.

8. Include a textual representation of the tree your algorithm produced for the Lenses dataset.

   four == 1
      THEN 3
   IF four == 2 AND
       IF three == 1 AND
   IF one == 3 AND
       two == 1
      THEN 3
   two == 2
      THEN 2
   one == 2
      THEN 2
   one == 1
      THEN 2
   IF three == 2 AND
       IF one == 2 AND
   two == 1
      THEN 1
   two == 2
      THEN 3
   one == 1
      THEN 1

9. Describe your results for the Voting data set. (e.g., What was the size of the tree? How did your implementation compare to existing implementations?)

   The results from the voting data were surprisingly good. Compared to
   algorithms that Weka was using I averaged 20% better results than Weka
   (Weka was getting 75% and I was getting 95%). It was nice because all the
   data was already nominal. I just needed to reverse the columns so that the
   targets were the last column of the data.

10. Include ___a portion of___ the representation of the tree your algorithm produced for the Voting dataset.

    IF physician-fee-freeze == ? AND
        education-spending == y
       THEN republican
    IF education-spending == ? AND
        crime == ?
       THEN democrat
    crime == y
       THEN republican
    crime == n
       THEN democrat
    education-spending == n
       THEN democrat
    IF physician-fee-freeze == n AND
        IF adoption-of-the-budget-resolution == n AND
    IF synfuels-corporation-cutback == n AND
        religious-groups-in-schools == y
       THEN democrat
    IF religious-groups-in-schools == n AND
        duty-free-exports == y
       THEN democrat
    duty-free-exports == n
       THEN republican
    synfuels-corporation-cutback == y
       THEN democrat
    adoption-of-the-budget-resolution == ?
       THEN democrat
    adoption-of-the-budget-resolution == y
       THEN democrat

11. If applicable, please describe anything you did to go above and beyond and the results you saw.

    I made my own functions to nominalize data, and I something that I was
    proud of was I did the entire project without making a class. I feel like
    lots of the Data Science things apply really well to the functional
    programming paradigm, and I tried to stick to that throughout my code.
    This isn't really going above and beyond, but I didn't look up anyone
    else's code while doing the project. For me that is always neat to see
    how I somewhat naturally come up with a working solution.

12. Please select the category you feel best describes your assignment:
A - Some attempt was made
B - Developing, but signficantly deficient
C - Slightly deficient, but still mostly adequate
*D* - Meets requirements
*E* - Shows creativity and excels above and beyond requirements

1. Provide a brief justification (1-2 sentences) for selecting that category.

   I think I made a decent effort in adding somethings that weren't
   necessary required, like nominalizing the data myself. I personally feel
   that although it isn't much, it's enough to get some recognition.d classifier on the Iris data-set and obtained a result.
       True
    7. What accuracy do you get when running the HardCoded classifier on the
       Iris data-set and why do you think that is? 
       - 33%, because the data-set is divided equally into 3 classes and my
         algorithm to predict is set to only predict one of the 3 classes, so
         it averages about 33%.




    8. Please select the category you feel best describes your assignment:
       1 - Some attempt was made
       2 - Developing, but significantly deficient
       3 - Slightly deficient, but still mostly adequate
       *4 - Meets requirements*
       5 - Shows creativity and excels above and beyond requirements

    1. Provide a brief justification (1-2 sentences) for selecting that
       category:
       1.  Although, I have yet to go above and beyond, my code fulfills
    all the requirements of the assignment

*** Submission 2
    When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

    1. Please provide the URL of your public GitHub repository.
       
    https://github.com/coljamkop/k-nearest-neighbors

    2. Briefly describe your overall approach to the task and highlight the most
       difficult part of this assignment.

    My overall approach was to get my classifier as generic as possible and then
    make up for it with data normalization. This most difficult thing was
    figuring out how to convert nominal data to numeric data in a clean fashion.
    In the end, I put everything in a set and then referenced the index of the
    set to numericise the data. This isn't great because it doesn't handle
    distances very well, but I ran out of time to do it any other way.

    3. Briefly describe how you handled the distance between nominal attributes.

       As of right now I just reference the index of the list representation of
       the set representation of the data. Had I had more time I was considering
       converting my nominal data to binary data, or trying to determine the
       bell curve of the data and use that to normalize the nominal data into
       numeric data.

    4. Briefly describe your process for handling numeric data on different scales (i.e., normalizing).

       Given more time, I would have scaled that data from 0 to 1 and weight based on a bell curve.

    5. Describe your results for the Iris data set. (For example, what level of accuracy did you see for different values of K? How did your implementation compare to existing implementations?)
       | k |  % |
       | 5 | 98 |
       | 3 | 96 |
       | 1 | 92 |

    6. Describe your results for the Car data set. (For example, what level of accuracy did you see for different values of K? How did your implementation compare to existing implementations?)
       
       Didn't finish

    7. Describe anything you did to go above and beyond the minimum standard requirements.


    8. Please select the category you feel best describes your assignment:
    A - Some attempt was made
    B - Developing, but signficantly deficient
    C - Slightly deficient, but still mostly adequate
    *D - Meets requirements*
    E - Shows creativity and excels above and beyond requirements


    9. Provide a brief justification (1-2 sentences) for selecting that category.
       
       Although I was unable to fully complete the assignment as specified, I
       think it meets the expectations discussed in class. Although I didn't do
       anything overly intense, I did find a lot of cool ways to make my code
       more succinct.
*** Code
    #+BEGIN_SRC python :tangle HardCodedClassifier.py
      class HardCodedClassifier(object):
          def __init__(self):
              pass
          def fit(self, inputVector, targetVector):
              pass
          def predict(self, inputVector):
              return [self.classify(x) for x in range(len(inputVector))]
          def classify(self, instance):
              return 0
    #+END_SRC
    
    #+BEGIN_SRC python :tangle KopsaClassifier.py
      import numpy
      class KopsaClassifier(object):
          def fit(self, inputvector, targetvector):
              self.inputvector = inputvector
              self.targetvector = targetvector
          def knn(self, instance, k):
              # find distance from instance for each element in inputvector
              distances = ((self.inputvector - instance)**2).sum(axis=1)
              # sort distances
              indices = numpy.argsort(distances, axis=0)
              nearestNeighbors = [self.targetvector[i] for i in indices[:k]]
              return nearestNeighbors
          def predict(self, inputvector):
              return [self.classify(x, 1) for x in inputvector]
          def classify(self, instance, k):
              nearestNeighbors = self.knn(instance, k)
              return max(set(nearestNeighbors), key=nearestNeighbors.count)
    #+END_SRC

    #+BEGIN_SRC python :tangle kNearestNeighbor.py :results output
      from sklearn import datasets
      from sklearn import model_selection
      from HardCodedClassifier import HardCodedClassifier
      from KopsaClassifier import KopsaClassifier


      def accuracy(output, target):
          truePositive = 0
          falsePositive = 0
          for i in range(len(output)):
              if output[i] == target[i]:
                  truePositive = truePositive + 1
              else:
                  falsePositive = falsePositive + 1
          return float(truePositive) / len(output)

      def run(inputVector, targetVector):
          # Shuffle input and target
          # knuth_shuffle(inputVector, targetVector)
          trainInput, testInput, trainTarget, testTarget = model_selection.train_test_split(inputVector,
                                                                            targetVector,
                                                                            test_size=0.33)
          classifier = KopsaClassifier()
          classifier.fit(trainInput, trainTarget)
          testOutput = classifier.predict(testInput)

          print accuracy(testOutput, testTarget)

      iris = datasets.load_iris()
      run(iris.data, iris.target)
    #+END_SRC

    #+BEGIN_SRC python :tangle irisDatasetImporter.py :results output
      import csv
      class IrisDataImporter(object):
         def __init__(self, filename):
            self.data = [row for row in csv.reader(open(filename, 'rb'))][:150]
            self.classes = list(set([row[4] for row in self.data]))
            self.targetVector = map(lambda x: self.classes.index(x), [row[4] for row in self.data])
            for x in range(0, len(data)):
               del self.data[x][len(data[x])-1]
               self.data[x] = map(float, self.data[x])

    #+END_SRC

* Week 03 - Decision Trees
** Prepare : Reading
*** Why use trees?
    - Computational costs of trees are low: /O/ ( log /N/ )
*** Information Theory
    - The mathematical study of quantifying, storing and transmitting information
    - 20 Questions is a good example where, when done correctly, over 500,000 
      animals can be represented by 20 bits (or 20 yes or no questions)
**** Entropy
     - A key to success in information theory is finding a feature that gives
       you the most information (has the highest entropy).
     - In the game of 20 questions, "Is it a cat?" has a lot lower entropy when
       compared to the question: "Is it a mammal?"
       - I think this is because the search space is greatly reduced with the
         higher entropy question/feature.
     - The key is to find features that split the data set as evenly as possible.
       - For example, if the feature is true/positive or false/negative in all
         examples, then the feature doesn't provide any additional information.
         - If all our examples are animals, then the feature "Has blood"
           provides no additional information along with the feature "is rock",
           unless this included a the Pokemon universe, in which case Geodude
           and his evolutions could be filtered out with that question.
     - When creating a decision tree, we use a greedy formula that looks for the
       highest entropy features as its nodes closest to the root.
     - Entropy can be calculated using the following formula:
     #+BEGIN_EXAMPLE
  def calc_entropy(p):
     if p!=0:
        return -p * np.log2(p)
     else:
        return 0
     #+END_EXAMPLE
** Submission
   When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.


   1. Please provide a link to your classifier in your public GitHub repo.
      https://github.com/coljamkop/Decision-Tree
   2. Briefly describe your overall approach to the task and highlight the most
      difficult part of this assignment.

      My overall approach was to break the project up into little pieces and
      then combine them all in the end. I started with an entropy function, then
      added the functionality to handle multiple arguments, then made a function
      that would find the entropy for a given feature, then for a given column,
      then find the column with the least entropy, and then use all of those
      functions to help build the decision tree.
      
      The hardest part by far was conceptualizing how the decision tree was
      going to be put together. I kept on thinking of the data structure that I
      would create in order to handle this idea, but then I realized that I
      didn't need that, I just need a dictionary. When I was able to realize how
      my dreams could be accomplished with a dictionary, things started falling
      together rather quickly.

   3. Briefly describe how you handled numeric data.

      Handling numeric data was pretty simple, I would nominalize it by creating
      bins to organize it by. Once you have the bin ranges for the data to be
      categorized then the /pandas/ library took care of the rest.

   4. Briefly describe your you handled missing data.
      
      If data was missing it would just move to a known good node in the tree
      and keep going from there. Definitely not the most elegant solution, but
      it worked, and my accuracy was average above 90%. What the value defaults
      to if their is no data is definitely something that could be fine-tuned in
      the future.

   5. Describe your results for the Iris data set. (e.g., What was the size of the tree? How did your implementation compare to existing implementations? How did your decision tree compare to your kNN classifier)
      
      In some cases, I was able to achieve 100%, and even with only using 10% of
      my data to train on, I would consistently get above 90%. So, that being
      said, the decision tree was more accurate than the kNN. 25 nodes excluding
      leaf nodes.

   6. Include a textual representation of the tree your algorithm produced for the iris dataset.
      
    IF petal_width == large AND
    IF sepal_length == medium AND
    sepal_width == very_small
       THEN Iris-virginica
    sepal_width == small
       THEN Iris-virginica
    IF sepal_width == medium AND
    petal_length == medium
       THEN Iris-versicolor
    petal_length == large
       THEN Iris-virginica
    sepal_length == very-large
       THEN Iris-virginica
    sepal_length == small
       THEN Iris-virginica
    IF sepal_length == large AND
    sepal_width == very_small
       THEN Iris-virginica
    sepal_width == small
       THEN Iris-versicolor
    petal_width == very-large
       THEN Iris-virginica
    petal_width == very_small
       THEN Iris-setosa
    IF petal_width == medium AND
    petal_length == small
       THEN Iris-versicolor
    IF petal_length == large AND
    IF sepal_length == medium AND
    sepal_width == small
       THEN Iris-virginica
    sepal_width == very_very_small
       THEN Iris-virginica
    sepal_length == large
       THEN Iris-virginica
    petal_length == medium
       THEN Iris-versicolor
    petal_width == small
       THEN Iris-versicolor
    petal_width == very_very_small
       THEN Iris-setosa
   
   7. Describe your results for the Lenses data set. (e.g., What was the size of the tree? How did your implementation compare to existing implementations?)

      The lenses data set performed fairly well considering the sparse amount of
      data. Giving it 80% of the data to train on and 20% to test on yielded
      great results, averaging around 90%, but fluctuating from 80% to 100%. I
      couldn't figure out how to get Weka to work with the lense data. It would
      give me poor numbers consistently. I assume it would compare well.

   8. Include a textual representation of the tree your algorithm produced for the Lenses dataset.

    four == 1
       THEN 3
    IF four == 2 AND
    IF three == 1 AND
    IF one == 3 AND
    two == 1
       THEN 3
    two == 2
       THEN 2
    one == 2
       THEN 2
    one == 1
       THEN 2
    IF three == 2 AND
    IF one == 2 AND
    two == 1
       THEN 1
    two == 2
       THEN 3
    one == 1
       THEN 1

   9. Describe your results for the Voting data set. (e.g., What was the size of the tree? How did your implementation compare to existing implementations?)

      The results from the voting data were surprisingly good. Compared to
      algorithms that Weka was using I averaged 20% better results than Weka
      (Weka was getting 75% and I was getting 95%). It was nice because all the
      data was already nominal. I just needed to reverse the columns so that the
      targets were the last column of the data.

   10. Include ___a portion of___ the representation of the tree your algorithm produced for the Voting dataset.

    IF physician-fee-freeze == ? AND
    education-spending == y
       THEN republican
    IF education-spending == ? AND
    crime == ?
       THEN democrat
    crime == y
       THEN republican
    crime == n
       THEN democrat
    education-spending == n
       THEN democrat
    IF physician-fee-freeze == n AND
    IF adoption-of-the-budget-resolution == n AND
    IF synfuels-corporation-cutback == n AND
    religious-groups-in-schools == y
       THEN democrat
    IF religious-groups-in-schools == n AND
    duty-free-exports == y
       THEN democrat
    duty-free-exports == n
       THEN republican
    synfuels-corporation-cutback == y
       THEN democrat
    adoption-of-the-budget-resolution == ?
       THEN democrat
    adoption-of-the-budget-resolution == y
       THEN democrat

   11. If applicable, please describe anything you did to go above and beyond and the results you saw.

       I made my own functions to nominalize data, and I something that I was
       proud of was I did the entire project without making a class. I feel like
       lots of the Data Science things apply really well to the functional
       programming paradigm, and I tried to stick to that throughout my code.
       This isn't really going above and beyond, but I didn't look up anyone
       else's code while doing the project. For me that is always neat to see
       how I somewhat naturally come up with a working solution.

   12. Please select the category you feel best describes your assignment:
   A - Some attempt was made
   B - Developing, but signficantly deficient
   C - Slightly deficient, but still mostly adequate
   *D* - Meets requirements
   *E* - Shows creativity and excels above and beyond requirements

   13. Provide a brief justification (1-2 sentences) for selecting that category.

       I think I made a decent effort in adding somethings that weren't
       necessary required, like nominalizing the data myself. I personally feel
       that although it isn't much, it's enough to get some recognition.
** DONE Code
   CLOSED: [2017-02-02 Thu 00:50]
*** Building the tree:
    It would be cool to build the tree recursively, but have it return the
    entire built tree. If it were to take in all the things it needed, and then
    create the necessary children from it.

    #+BEGIN_SRC python :results output :tangle ID3_Tree.py
      import pandas as pd
      import numpy as np
      def calc_entropy(*probabilities):
          return sum([0 if p == 0 else -p * np.log2(p) for p in probabilities])
      def nominalize_dataframe(df, numBins, labels):
          tempdataframe = df.copy(True)
          for column in list(tempdataframe)[:-1]:
              tempdataframe[column] = nominalize_column(df[column], numBins, labels)
          return tempdataframe
      def nominalize_column(column, numBins, labels):
          if not isinstance(column, str):
              return pd.cut(column, create_bins(column, numBins), labels=labels)
      def create_bins(column, numBins):
          columnMin = float(min(column))
          columnMin = columnMin - columnMin/10
          columnMax = float(max(column))
          return np.linspace(columnMin, columnMax, numBins)
      def getColumnEntropy(df, column_name):
          column_entropy_sum = 0
          for feature in set(df[column_name]):
              column_entropy_sum += getFeatureEntropy(df, column_name, feature)
          return column_entropy_sum
      def getFeatureEntropy(df, column_name, feature):
          counts = df[df[column_name] == feature].ix[:, -1].value_counts()
          entropy = calc_entropy(*list(counts / sum(counts)))
          ratio = list(df[column_name]).count(feature) / float(df[column_name].count())
          return entropy * ratio
      def getBestColumn(df):
          return list(df)[np.argmin(map(lambda(x): getColumnEntropy(df, x), list(df)[:-1]))]
      def buildTree(df):
          best_column = getBestColumn(df)
          d_tree = dict()
          for feature in set(df[best_column]):
              if len(list(df)) == 2 or 0.0 == getFeatureEntropy(df, best_column, feature):
                  d_tree[best_column, feature] = list(df[df[best_column] == feature].ix[:, -1])[0]
              else:
                  d_tree[best_column, feature] = buildTree(df[df[best_column] == feature].drop([best_column], axis=1))
          return d_tree
      def classifyDataFrame(df, d_tree):
          return [classifyRow(row, d_tree) for index, row in df.iterrows()]
      def classifyRow(row, d_tree):
          current_column = d_tree.keys()[0][0]
          classification = d_tree.get((current_column, row[current_column]), d_tree[d_tree.keys()[0][0], d_tree.keys()[0][1]])
          if not isinstance(classification, dict):
              return classification
          else:
              return classifyRow(row, classification)
      def accuracy(output, target):
          truePositive = 0
          falsePositive = 0
          for i in range(len(output)):
              if output[i] == target[i]:
                  truePositive = truePositive + 1
              else:
                  falsePositive = falsePositive + 1
          return float(truePositive) / len(output)
      def displayDTree(tree):
          for key in tree.keys():
              if isinstance(tree[key], dict):
                  print "IF", key[0], "==", key[1], "AND"
                  displayDTree(tree[key])
              else:
                  print key[0], "==", key[1]
                  print "  ", "THEN", tree[key]

      df0 = pd.read_csv("iris.data", header = None, names = ["sepal_length",
                                                            "sepal_width",
                                                            "petal_length",
                                                            "petal_width", "class"])
      names = ["very_very_small", "very_small", "small", "medium", "large", "very-large"]
      nomdf0 = nominalize_dataframe(df0, len(names)+1, names)
      d_tree0 = buildTree(nomdf0.sample(frac=.8))
      test_data0 = nomdf0.sample(frac=.2)
      output0 = classifyDataFrame(test_data0, d_tree0)
      target0 = list(test_data0["class"])
      print accuracy(output0, target0)

      df1 = pd.read_csv("lenses.data", sep='  ', names=["one", "two", "three", "four", "five"])
      d_tree1 = buildTree(df1.sample(frac=.8))
      test_data1 = df1.sample(frac=.2)
      output1 = classifyDataFrame(test_data1, d_tree1)
      target1 = list(test_data1.ix[:, -1])
      print accuracy(output1, target1)

      df2 = pd.read_csv("votes.data", names= ["Class Name",
                                              "handicapped-infants", "water-project-cost-sharing",
                                              "adoption-of-the-budget-resolution", "physician-fee-freeze", "el-salvador-aid",
                                              "religious-groups-in-schools", "anti-satellite-test-ban",
                                              "aid-to-nicaraguan-contras", "mx-missile", "immigration",
                                              "synfuels-corporation-cutback", "education-spending", "superfund-right-to-sue",
                                              "crime", "duty-free-exports", "export-administration-act-south-africa"])
      df2 = df2[list(df2)[::-1]]
      d_tree2 = buildTree(df2.sample(frac=.8))
      test_data2 = df2.sample(frac=.2)
      output2 = classifyDataFrame(test_data2, d_tree2)
      target2 = list(test_data2.ix[:, -1])
      print accuracy(output2, target2)
      displayDTree(d_tree2)
    #+END_SRC

* Week 04 - Neural Networks
** Prepare : Reading
*** How do animals think?
   The brain is filled with neurons that fire when certain conditions are met.
*** What's Hebb's Rule?
   If multiple neurons fire simultaneously the synaptic connection between them
   grows stronger.
*** What's McCulloch and Pitts Neurons?
   Weighted inputs are put into an adder and then if the sum exceeds a threshold
   the neuron fires.
*** Is this realistic?
    - Not really, this is oversimplified. 
    - The main difference is that real neurons fire a spike train that encodes
      information, and not just an binary signal.
    - M&C can have negative weights, which has not been seen in real neurons.
*** So... neural networks. How do they classify things? How do they learn?
    - One word: pattern recognition.
      - Assuming that there is some pattern in the data, then by showing the
        neural network a few examples, we hope that it will find the pattern and
        predict the other examples correctly.
    - The learning of the neural network doesn't happen in the neurons
      themselves, rather it takes place between the neurons.
    - The only values we are allowed to change are the weights and threshold for
      firing.
*** What is the Perceptron?
   - A simple neural network based off of M&P neurons.
   - /η/ defines the learning rate of the neural network.
     
*** What is the Multi-layer Perceptron?
*** How do neural networks get around linear classification?
    The use a put a layer of neurons between the input nodes and output nodes.
*** What does the "hidden layer" do in a MLP?
    It allows use to classify more complex data by adding more neurons to the Perceptron.
*** What is back-propagation?
    Back propagation is the idea of sending the errors of the neural networks
    back through the hidden layer to identify what went wrong where. This is
    trying to take a calculated approach to the adjusting the weights of the
    network to improve accuracy.
*** What problem does chain rule of differentiation solve?
    
*** What is the algorithm for the MLP?
*** To what values should weights be initialized to?
*** When does the MLP stop learning?
** Submission 1  
When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

1. Please provide a link to your classifier in your public GitHub repo.
https://github.com/coljamkop/neural-network   
   
For each of the following questions, indicate "True" or "False" on the line after the question.

2. A data structure is created to hold a node (a.k.a. neuron)
True
3. A set of input weights is stored for each node.
True
4. A layer of nodes (of any number) can be easily created.
True
5. Bias input is accounted for in each node.
True
6. Input can be taken for an actual dataset instance (e.g., Iris) and each node in the layer produces the appropriate value (0 or 1) according to its weights.
True
7. Data can be loaded for both the Iris and the Pima Indian Dataset.
True
8. Data is appropriately normalized before being given to the Neural Network.
True
** Submission 2
When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.


1. Please provide a link to your classifier in your public GitHub repo.
https://github.com/coljamkop/neural-network

For each of the following questions, indicate "True" or "False" on the line after the question.

2. Classifier can be built with an arbitrary number of layers and nodes.
True

3. The number of weights for the first layer is determined by the number of attributes.
True

4. Biases are present at every layer.
True

5. Values can be computed at the output layer (by computing the value of every node at every layer)
True

6. A sigmoid function is used for the activation function of each node.
True

7. An instance can be classified (albeit not very accurately) with the network.
True

8. A complete dataset (e.g., iris) can be classified and the overall accuracy reported.
True

** TODO Code
#+BEGIN_SRC python :results output :tangle neural_net.py
  import pandas as pd
  import numpy as np
  import random

  def create_layer(numNeurons, numInputs):
      return [[random.uniform(-1.0, 1.0) for x in range(numInputs + 1)] for x in range(numNeurons)]
  def calc_threshold(input_vector, input_weights):
      return sum([x * y for x, y in zip([-1] + input_vector, input_weights)])
  def calc_activation(threshold):
      return 1/(1 + 2.71828**-threshold)
  def calc_error_output(activation, target):
      return activation * (1 - activation) * (activation - target)
  def calc_error_hidden(activation, weights, error):
      return activation * (1 - activation) * sum(map(lambda x,y:x*y, weights, error)) 
  def calc_weight(current_weight, learning_rate, error, activation):
      new_weight = list(np.subtract(current_weight, (learning_rate * error * activation)))
      return new_weight
  def create_classifier(numInputs, *layers):
      layer = layers[0]
      if len(layers) == 1:
          return [create_layer(layer, numInputs)]
      else:
          return [create_layer(layer, numInputs)] + create_classifier(layer, *layers[1:])
  def train(inputVector, targetVector, neural_net, learningRate):
      current_output = map(lambda neuron_weights:
                           calc_activation(calc_threshold(inputVector, neuron_weights)),
                           neural_net[0])
      if len(neural_net) == 1:
          j_error = map(lambda x:calc_error_output(x[0], x[1]), zip(current_output, targetVector))
          return [map(lambda x:calc_weight(x[0], learningRate, x[1], x[2]), zip(neural_net[0], j_error, current_output))], j_error
      else:
          jk_weights, k_error = train(current_output, targetVector, neural_net[1:], learningRate)
          j_error = map(lambda x,y:calc_error_hidden(x, y, k_error), current_output, zip(*jk_weights[0])[1:])
          return [map(lambda x:calc_weight(x[0], learningRate, x[1], x[2]), zip(neural_net[0], j_error, inputVector))] + jk_weights, j_error
  def classify(inputVector, neural_net):
      current_output = map(lambda neuron_weights:
                           calc_activation(calc_threshold(inputVector, neuron_weights)),
                           neural_net[0])
      if len(neural_net) == 1:
          return current_output
      else:
          return classify(current_output, neural_net[1:])
  def accuracy(output, target):
      truePositive = 0
      falsePositive = 0
      for i in range(len(output)):
          if output[i] == target[i]:
              truePositive = truePositive + 1
          else:
              falsePositive = falsePositive + 1
      return float(truePositive) / len(output)
  def outputize(num, length):
      output = [0] * length
      output[num-1] = 1
      return output
  df = pd.read_csv("iris.data", header = None, names = ["sepal_length",
                                                        "sepal_width",
                                                        "petal_length",
                                                        "petal_width", "class"])
  df = df.sample(frac=1)
  df_input = df.drop("class", axis=1)
  df_norm = (df_input - df_input.mean()) / (df_input.max() - df_input.min())
  
  classifier = create_classifier(4, 4, 4, 3)
  classes = list(set(df["class"]))
  targetVector = [classes.index(x) for x in df["class"]]
  for i in range(200):
      for index, row in df_norm.iterrows():
          classifier = train(row, outputize(targetVector[index], 3), classifier, 1)[:-1][0]
  print classifier[0]
  output = map(np.argmax, [classify(row, classifier) for index, row in df_norm.iterrows()])
  print output
  print accuracy(output, targetVector)

  df = pd.read_csv("diabetes.data", header = None)
  df_input = df.drop(list(df)[-1], axis=1)
  df_norm = (df_input - df_input.mean()) / (df_input.max() - df_input.min())
  df_norm
  #+END_SRC

  #+RESULTS:
  : [[1.9610874873261708, 0.57700853888113346, 2.053164749702951, 1.6164381574091269, 0.67081922287565976], [-1.2779131379153537, -0.71118027293140862, -0.39979359474776099, -1.4567157827571404, -0.95045808609737892], [1.3370184927705284, 2.1744893163751398, 0.81584435013646472, 1.9930164499280769, 1.9985675592156906], [0.47826479897094459, 0.88203642012603778, 0.12447573351835133, 0.2183090771996144, 1.6154870640187202]]
  : [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]
  : 0.333333333333

* Python Thought of the Day
** Code
   #+BEGIN_SRC python :results output
  class myClass(object):
      def __init__(self, x):
          self._x = x

      @property
      def x(self):
          print "Calling the getter for my \"private\" member variable"
          return self._x

      @x.setter
      def x(self, x):
          print "Calling the setter for my \"private\" member variable"
          self._x = x

      @x.deleter
      def x(self):
          print "Calling the deleter for my \"private\" member variable"
          del self._x

  def example():
      classy = myClass(20)
      print(classy.x)
      classy.x = 30
      print(classy.x)
      del classy.x

  def main():
      print("Starting off the example")
      example()
      print("Ending the example")
  main()
   #+END_SRC

   #+RESULTS:
   : Starting off the example
   : Calling the getter for my "private" member variable
   : 20
   : Calling the setter for my "private" member variable
   : Calling the getter for my "private" member variable
   : 30
   : Calling the deleter for my "private" member variable
   : Ending the example
** Code
#+BEGIN_SRC python results:output

#+END_SRC
* Team Project
** Data Understanding
*** What is the overall objective of your project?
    To find patterns in students' academic progression associated with
    graduating quickly and within the credit limit.
*** Where/how are you going to obtain the data? For example, do you need to scrape the data from a web source, contact a company or individual for access/permission?
    We will receive the data from Br. Bergstrom through Br. Burton.
*** Are there any copyright or terms of service limitations that may affect your ability to work with the data?
    Only having access to one copy of the data could potentially limit data, but
    we are working with Br. Burton to get that sorted out.
*** What type of machine learning algorithm / process do you anticipate using (e.g., classification, regression, clustering) and if applicable, what variable(s), in particular, will you be trying to classify, etc. 
    Classification through a decision tree, because knowing whether or not a
    student graduates is not as valuable as knowing why they graduate. We'll be
    looking at the order students take classes and how that influences their
    graduation, grades, failing a class, repeating a class, changing majors,
    marital status, serving a mission, prerequisites.
*** How many and what type of attributes do you anticipate? (e.g., less than than 10 vs. 50-100, numeric vs. nominal, time-series data, etc.)
    Approximately a total of 20 attributes.
*** How many instances or records do you anticipate?
    6000 students + 240000 courses taken + 4000 graduated students
*** What do you see as the biggest risk to the success of your project?
    The biggest risk for us is asking too broad of a question could lead to
    difficulty in discovering good/solid conclusions. Another challenge is
    filtering the data to have relevant information to work with.
