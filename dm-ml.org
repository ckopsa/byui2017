#+TITLE: Data Mining and Machine Learning
#+DATE: <2017-01-06 Fri>
#+AUTHOR: Colton Kopsa
#+EMAIL: Aghbac@Aghbac.local
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not "LOGBOOK") date:t
#+OPTIONS: e:t email:nil f:t inline:t num:t p:nil pri:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t toc:t todo:t |:t
#+CREATOR: Emacs 25.1.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* Week 01 - Introduction to Machine Learning
* Week 02 - k-Nearest Neighbors
** 01 PROVE : ASSIGNMENT - EXPERIMENT SHELL & HARDCODED CLASSIFIER
*** Submission
    When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

    Name: Colton Kopsa

    1. Please provide the URL of your public GitHub repository.

       https://github.com/coljamkop/k-nearest-neighbors

    For questions 2-6, please type "True" or "False" in front of the question number.

    2. My experiment shell can correctly load the Iris data-set:
       True
    3. My experiment shell randomizes the order of the instances (making sure to keep instances lined up with their appropriate targets) it and splits the data into a training set (70%) and a test set (30%)?
       True
    4. I have created a HardCoded classifier class with two methods: train and predict. The train method accepts training data (including targets). The predict method returns a prediction or classification for each instance it receives.
       True 
    5. The Experime  
   print calc_entropy(2/3.0, 1/3.0)
nt Shell, processes the data, passes the training data to the classifier's€™s train method, the test data to the predict method, and then compares the predicted values against the correct answers, to produce an overall accuracy (on the test set).
       True
    6. I have run the HardCoded classifier on the Iris data-set and obtained a result.
       True
    7. What accuracy do you get when running the HardCoded classifier on the
       Iris data-set and why do you think that is? 
       - 33%, because the data-set is divided equally into 3 classes and my
         algorithm to predict is set to only predict one of the 3 classes, so
         it averages about 33%.




    8. Please select the category you feel best describes your assignment:
       1 - Some attempt was made
       2 - Developing, but significantly deficient
       3 - Slightly deficient, but still mostly adequate
       *4 - Meets requirements*
       5 - Shows creativity and excels above and beyond requirements

    1. Provide a brief justification (1-2 sentences) for selecting that
       category:
       1.  Although, I have yet to go above and beyond, my code fulfills
    all the requirements of the assignment

*** Submission 2
    When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

    1. Please provide the URL of your public GitHub repository.
       
    https://github.com/coljamkop/k-nearest-neighbors

    2. Briefly describe your overall approach to the task and highlight the most
       difficult part of this assignment.

    My overall approach was to get my classifier as generic as possible and then
    make up for it with data normalization. This most difficult thing was
    figuring out how to convert nominal data to numeric data in a clean fashion.
    In the end, I put everything in a set and then referenced the index of the
    set to numericise the data. This isn't great because it doesn't handle
    distances very well, but I ran out of time to do it any other way.

    3. Briefly describe how you handled the distance between nominal attributes.

       As of right now I just reference the index of the list representation of
       the set representation of the data. Had I had more time I was considering
       converting my nominal data to binary data, or trying to determine the
       bell curve of the data and use that to normalize the nominal data into
       numeric data.

    4. Briefly describe your process for handling numeric data on different scales (i.e., normalizing).

       Given more time, I would have scaled that data from 0 to 1 and weight based on a bell curve.

    5. Describe your results for the Iris data set. (For example, what level of accuracy did you see for different values of K? How did your implementation compare to existing implementations?)
       | k |  % |
       | 5 | 98 |
       | 3 | 96 |
       | 1 | 92 |

    6. Describe your results for the Car data set. (For example, what level of accuracy did you see for different values of K? How did your implementation compare to existing implementations?)
       
       Didn't finish

    7. Describe anything you did to go above and beyond the minimum standard requirements.


    8. Please select the category you feel best describes your assignment:
    A - Some attempt was made
    B - Developing, but signficantly deficient
    C - Slightly deficient, but still mostly adequate
    *D - Meets requirements*
    E - Shows creativity and excels above and beyond requirements


    9. Provide a brief justification (1-2 sentences) for selecting that category.
       
       Although I was unable to fully complete the assignment as specified, I
       think it meets the expectations discussed in class. Although I didn't do
       anything overly intense, I did find a lot of cool ways to make my code
       more succinct.
*** Code
    #+BEGIN_SRC python :tangle HardCodedClassifier.py
      class HardCodedClassifier(object):
          def __init__(self):
              pass
          def fit(self, inputVector, targetVector):
              pass
          def predict(self, inputVector):
              return [self.classify(x) for x in range(len(inputVector))]
          def classify(self, instance):
              return 0
    #+END_SRC
    
    #+BEGIN_SRC python :tangle KopsaClassifier.py
      import numpy
      class KopsaClassifier(object):
          def fit(self, inputvector, targetvector):
              self.inputvector = inputvector
              self.targetvector = targetvector
          def knn(self, instance, k):
              # find distance from instance for each element in inputvector
              distances = ((self.inputvector - instance)**2).sum(axis=1)
              # sort distances
              indices = numpy.argsort(distances, axis=0)
              nearestNeighbors = [self.targetvector[i] for i in indices[:k]]
              return nearestNeighbors
          def predict(self, inputvector):
              return [self.classify(x, 1) for x in inputvector]
          def classify(self, instance, k):
              nearestNeighbors = self.knn(instance, k)
              return max(set(nearestNeighbors), key=nearestNeighbors.count)
    #+END_SRC

    #+BEGIN_SRC python :tangle kNearestNeighbor.py :results output
      from sklearn import datasets
      from sklearn import model_selection
      from HardCodedClassifier import HardCodedClassifier
      from KopsaClassifier import KopsaClassifier


      def accuracy(output, target):
          truePositive = 0
          falsePositive = 0
          for i in range(len(output)):
              if output[i] == target[i]:
                  truePositive = truePositive + 1
              else:
                  falsePositive = falsePositive + 1
          return float(truePositive) / len(output)

      def run(inputVector, targetVector):
          # Shuffle input and target
          # knuth_shuffle(inputVector, targetVector)
          trainInput, testInput, trainTarget, testTarget = model_selection.train_test_split(inputVector,
                                                                            targetVector,
                                                                            test_size=0.33)
          classifier = KopsaClassifier()
          classifier.fit(trainInput, trainTarget)
          testOutput = classifier.predict(testInput)

          print accuracy(testOutput, testTarget)

      iris = datasets.load_iris()
      run(iris.data, iris.target)
    #+END_SRC

    #+BEGIN_SRC python :tangle irisDatasetImporter.py :results output
      import csv
      class IrisDataImporter(object):
         def __init__(self, filename):
            self.data = [row for row in csv.reader(open(filename, 'rb'))][:150]
            self.classes = list(set([row[4] for row in self.data]))
            self.targetVector = map(lambda x: self.classes.index(x), [row[4] for row in self.data])
            for x in range(0, len(data)):
               del self.data[x][len(data[x])-1]
               self.data[x] = map(float, self.data[x])

    #+END_SRC

* Week 03 - Decision Trees
** Prepare : Reading
*** Why use trees?
    - Computational costs of trees are low: /O/ ( log /N/ )
*** Information Theory
    - The mathematical study of quantifying, storing and transmitting information
    - 20 Questions is a good example where, when done correctly, over 500,000 
      animals can be represented by 20 bits (or 20 yes or no questions)
**** Entropy
     - A key to success in information theory is finding a feature that gives
       you the most information (has the highest entropy).
     - In the game of 20 questions, "Is it a cat?" has a lot lower entropy when
       compared to the question: "Is it a mammal?"
       - I think this is because the search space is greatly reduced with the
         higher entropy question/feature.
     - The key is to find features that split the data set as evenly as possible.
       - For example, if the feature is true/positive or false/negative in all
         examples, then the feature doesn't provide any additional information.
         - If all our examples are animals, then the feature "Has blood"
           provides no additional information along with the feature "is rock",
           unless this included a the Pokemon universe, in which case Geodude
           and his evolutions could be filtered out with that question.
     - When creating a decision tree, we use a greedy formula that looks for the
       highest entropy features as its nodes closest to the root.
     - Entropy can be calculated using the following formula:
     #+BEGIN_EXAMPLE
  def calc_entropy(p):
     if p!=0:
        return -p * np.log2(p)
     else:
        return 0
     #+END_EXAMPLE
** COMMENT Code
   #+BEGIN_SRC python :tangle IrisDatasetImporter.py
     import csv
     import copy
     class IrisDataImporter(object):
        def __init__(self, filename):
           self.data = [row for row in csv.reader(open(filename, 'rb'))][:150]
           self.classes = list(set([row[4] for row in self.data]))
           self.inputVector = copy.deepcopy(self.data)
           self.targetVector = map(lambda x: self.classes.index(x), [row[4] for row in self.data])
           for x in range(0, len(self.inputVector)):
              del self.inputVector[x][len(self.inputVector[x])-1]
              self.inputVector[x] = map(float, self.inputVector[x])

    #+END_SRC
1. sepal length in cm 
2. sepal width in cm 
3. petal length in cm 
4. petal width in cm 
5. class: 
   #+BEGIN_SRC python :results output
     import pandas as pd
     import numpy as np
     from IrisDatasetImporter import IrisDataImporter

     def calc_entropy(*probabilities):
         return sum([0 if p == 0 else -p * np.log2(p) for p in probabilities])
     def calc_info_gain(data, target, feature):
        pass
     def nominalize_dataframe(dataframe, numBins, labels):
         tempdataframe = dataframe.copy(True)
         for column in list(tempdataframe)[:-1]:
             tempdataframe[column] = nominalize_column(df[column], numBins, labels)
         return tempdataframe
     def nominalize_column(column, numBins, labels):
         if not isinstance(column[0], str):
             return pd.cut(column, create_bins(column, numBins), labels=labels)
     def create_bins(column, numBins):
         columnMin = float(min(column))
         columnMin = columnMin - columnMin/10
         columnMax = float(max(column))
         return np.linspace(columnMin, columnMax, numBins)

     df = pd.read_csv("iris.data", header = None, names = ["sepal_length", "sepal_width", "petal_length", "petal_width", "class"])
     names = ["very_small", "small", "medium", "large", "very-large"]
     nomdf = nominalize_dataframe(df, 6, names)
     # Get Columns Ratios
     for column in list(nomdf)[:-1]:
         for feature in set(nomdf[column]):
             print column, feature
             counts = nomdf[nomdf[column] == feature]["class"].value_counts()
             print calc_entropy(*list(counts / sum(counts))) * nomdf[nomdf[column] == feature][column].count() / float(nomdf[column].count())
         map(lambda(x): x / float(len(nomdf[column])), list(nomdf[column].value_counts()))
   #+END_SRC

         #+RESULTS:
         #+begin_example
         sepal_length small
         0.211348729306
         sepal_length large
         0.244875419216
         sepal_length very_small
         0.0
         sepal_length medium
         0.39486511785
         sepal_length very-large
         0.0
         sepal_width large
         0.170386515843
         sepal_width very-large
         0.0
         sepal_width very_small
         0.0566014999712
         sepal_width medium
         0.702656164824
         sepal_width small
         0.258889137542
         petal_length large
         0.275318007875
         petal_length small
         0.0
         petal_length very_small
         0.0
         petal_length medium
         0.0433915377579
         petal_length very-large
         0.0
         petal_width small
         0.0289901036373
         petal_width large
         0.12821805578
         petal_width very_small
         0.0
         petal_width medium
         0.103223327842
         petal_width very-large
         0.0
#+end_example

* Python Thought of the Day
** Code
   #+BEGIN_SRC python :results output
  class myClass(object):
      def __init__(self, x):
          self._x = x

      @property
      def x(self):
          print "Calling the getter for my \"private\" member variable"
          return self._x

      @x.setter
      def x(self, x):
          print "Calling the setter for my \"private\" member variable"
          self._x = x

      @x.deleter
      def x(self):
          print "Calling the deleter for my \"private\" member variable"
          del self._x

  def example():
      classy = myClass(20)
      print(classy.x)
      classy.x = 30
      print(classy.x)
      del classy.x

  def main():
      print("Starting off the example")
      example()
      print("Ending the example")
  main()
   #+END_SRC

   #+RESULTS:
   : Starting off the example
   : Calling the getter for my "private" member variable
   : 20
   : Calling the setter for my "private" member variable
   : Calling the getter for my "private" member variable
   : 30
   : Calling the deleter for my "private" member variable
   : Ending the example
