#+TITLE: Data Mining and Machine Learning
#+DATE: <2017-01-06 Fri>
#+AUTHOR: Colton Kopsa
#+EMAIL: Aghbac@Aghbac.local
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not "LOGBOOK") date:t
#+OPTIONS: e:t email:nil f:t inline:t num:t p:nil pri:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t toc:t todo:t |:t
#+CREATOR: Emacs 25.1.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* Week 01 - Introduction to Machine Learning
* Week 02 - k-Nearest Neighbors
** 01 PROVE : ASSIGNMENT - EXPERIMENT SHELL & HARDCODED CLASSIFIER
*** Submission
    When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

    Name: Colton Kopsa

    1. Please provide the URL of your public GitHub repository.

       https://github.com/coljamkop/k-nearest-neighbors

    For questions 2-6, please type "True" or "False" in front of the question number.

    2. My experiment shell can correctly load the Iris data-set:
       True
    3. My experiment shell randomizes the order of the instances (making sure to keep instances lined up with their appropriate targets) it and splits the data into a training set (70%) and a test set (30%)?
       True
    4. I have created a HardCoded classifier class with two methods: train and predict. The train method accepts training data (including targets). The predict method returns a prediction or classification for each instance it receives.
       True 
    5. The Experime  
    print calc_entropy(2/3.0, 1/3.0)
    nt Shell, processes the data, passes the training data to the classifier's€™s train method, the test data to the predict method, and then compares the predicted values against the correct answers, to produce an overall accuracy (on the test set).
    True
    6. I have run the HardCoded classifier on the Iris data-set and obtained a result.
       True
    7. What accuracy do you get when running the HardCoded classifier on the
       Iris data-set and why do you think that is? 
       - 33%, because the data-set is divided equally into 3 classes and my
         algorithm to predict is set to only predict one of the 3 classes, so
         it averages about 33%.




    8. Please select the category you feel best describes your assignment:
       1 - Some attempt was made
       2 - Developing, but significantly deficient
       3 - Slightly deficient, but still mostly adequate
       *4 - Meets requirements*
       5 - Shows creativity and excels above and beyond requirements

    1. Provide a brief justification (1-2 sentences) for selecting that
       category:
       1.  Although, I have yet to go above and beyond, my code fulfills
    all the requirements of the assignment

*** Submission 2
    When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

    1. Please provide the URL of your public GitHub repository.
       
    https://github.com/coljamkop/k-nearest-neighbors

    2. Briefly describe your overall approach to the task and highlight the most
       difficult part of this assignment.

    My overall approach was to get my classifier as generic as possible and then
    make up for it with data normalization. This most difficult thing was
    figuring out how to convert nominal data to numeric data in a clean fashion.
    In the end, I put everything in a set and then referenced the index of the
    set to numericise the data. This isn't great because it doesn't handle
    distances very well, but I ran out of time to do it any other way.

    3. Briefly describe how you handled the distance between nominal attributes.

       As of right now I just reference the index of the list representation of
       the set representation of the data. Had I had more time I was considering
       converting my nominal data to binary data, or trying to determine the
       bell curve of the data and use that to normalize the nominal data into
       numeric data.

    4. Briefly describe your process for handling numeric data on different scales (i.e., normalizing).

       Given more time, I would have scaled that data from 0 to 1 and weight based on a bell curve.

    5. Describe your results for the Iris data set. (For example, what level of accuracy did you see for different values of K? How did your implementation compare to existing implementations?)
       | k |  % |
       | 5 | 98 |
       | 3 | 96 |
       | 1 | 92 |

    6. Describe your results for the Car data set. (For example, what level of accuracy did you see for different values of K? How did your implementation compare to existing implementations?)
       
       Didn't finish

    7. Describe anything you did to go above and beyond the minimum standard requirements.


    8. Please select the category you feel best describes your assignment:
    A - Some attempt was made
    B - Developing, but signficantly deficient
    C - Slightly deficient, but still mostly adequate
    *D - Meets requirements*
    E - Shows creativity and excels above and beyond requirements


    9. Provide a brief justification (1-2 sentences) for selecting that category.
       
       Although I was unable to fully complete the assignment as specified, I
       think it meets the expectations discussed in class. Although I didn't do
       anything overly intense, I did find a lot of cool ways to make my code
       more succinct.
*** Code
    #+BEGIN_SRC python :tangle HardCodedClassifier.py
      class HardCodedClassifier(object):
          def __init__(self):
              pass
          def fit(self, inputVector, targetVector):
              pass
          def predict(self, inputVector):
              return [self.classify(x) for x in range(len(inputVector))]
          def classify(self, instance):
              return 0
    #+END_SRC
    
    #+BEGIN_SRC python :tangle KopsaClassifier.py
      import numpy
      class KopsaClassifier(object):
          def fit(self, inputvector, targetvector):
              self.inputvector = inputvector
              self.targetvector = targetvector
          def knn(self, instance, k):
              # find distance from instance for each element in inputvector
              distances = ((self.inputvector - instance)**2).sum(axis=1)
              # sort distances
              indices = numpy.argsort(distances, axis=0)
              nearestNeighbors = [self.targetvector[i] for i in indices[:k]]
              return nearestNeighbors
          def predict(self, inputvector):
              return [self.classify(x, 1) for x in inputvector]
          def classify(self, instance, k):
              nearestNeighbors = self.knn(instance, k)
              return max(set(nearestNeighbors), key=nearestNeighbors.count)
    #+END_SRC

    #+BEGIN_SRC python :tangle kNearestNeighbor.py :results output
      from sklearn import datasets
      from sklearn import model_selection
      from HardCodedClassifier import HardCodedClassifier
      from KopsaClassifier import KopsaClassifier


      def accuracy(output, target):
          truePositive = 0
          falsePositive = 0
          for i in range(len(output)):
              if output[i] == target[i]:
                  truePositive = truePositive + 1
              else:
                  falsePositive = falsePositive + 1
          return float(truePositive) / len(output)

      def run(inputVector, targetVector):
          # Shuffle input and target
          # knuth_shuffle(inputVector, targetVector)
          trainInput, testInput, trainTarget, testTarget = model_selection.train_test_split(inputVector,
                                                                            targetVector,
                                                                            test_size=0.33)
          classifier = KopsaClassifier()
          classifier.fit(trainInput, trainTarget)
          testOutput = classifier.predict(testInput)

          print accuracy(testOutput, testTarget)

      iris = datasets.load_iris()
      run(iris.data, iris.target)
    #+END_SRC

    #+BEGIN_SRC python :tangle irisDatasetImporter.py :results output
      import csv
      class IrisDataImporter(object):
         def __init__(self, filename):
            self.data = [row for row in csv.reader(open(filename, 'rb'))][:150]
            self.classes = list(set([row[4] for row in self.data]))
            self.targetVector = map(lambda x: self.classes.index(x), [row[4] for row in self.data])
            for x in range(0, len(data)):
               del self.data[x][len(data[x])-1]
               self.data[x] = map(float, self.data[x])

    #+END_SRC

* Week 03 - Decision Trees
** Prepare : Reading
*** Why use trees?
    - Computational costs of trees are low: /O/ ( log /N/ )
*** Information Theory
    - The mathematical study of quantifying, storing and transmitting information
    - 20 Questions is a good example where, when done correctly, over 500,000 
      animals can be represented by 20 bits (or 20 yes or no questions)
**** Entropy
     - A key to success in information theory is finding a feature that gives
       you the most information (has the highest entropy).
     - In the game of 20 questions, "Is it a cat?" has a lot lower entropy when
       compared to the question: "Is it a mammal?"
       - I think this is because the search space is greatly reduced with the
         higher entropy question/feature.
     - The key is to find features that split the data set as evenly as possible.
       - For example, if the feature is true/positive or false/negative in all
         examples, then the feature doesn't provide any additional information.
         - If all our examples are animals, then the feature "Has blood"
           provides no additional information along with the feature "is rock",
           unless this included a the Pokemon universe, in which case Geodude
           and his evolutions could be filtered out with that question.
     - When creating a decision tree, we use a greedy formula that looks for the
       highest entropy features as its nodes closest to the root.
     - Entropy can be calculated using the following formula:
     #+BEGIN_EXAMPLE
  def calc_entropy(p):
     if p!=0:
        return -p * np.log2(p)
     else:
        return 0
     #+END_EXAMPLE
** TODO Code
*** Building the tree:
    It would be cool to build the tree recursively, but have it return the
    entire built tree. If it were to take in all the things it needed, and then
    create the necessary children from it.

    #+BEGIN_SRC python :tangle Branch.py
      class Branch(Object):
          self.children = []
    

    #+END_SRC

    #+BEGIN_SRC python :results output
     import pandas as pd
     import numpy as np
     def calc_entropy(*probabilities):
         return sum([0 if p == 0 else -p * np.log2(p) for p in probabilities])
     def calc_info_gain(data, target, feature):
         pass
     def nominalize_dataframe(dataframe, numBins, labels):
         tempdataframe = dataframe.copy(True)
         for column in list(tempdataframe)[:-1]:
             tempdataframe[column] = nominalize_column(df[column], numBins, labels)
             return tempdataframe
     def nominalize_column(column, numBins, labels):
         if not isinstance(column[0], str):
             return pd.cut(column, create_bins(column, numBins), labels=labels)
     def create_bins(column, numBins):
         columnMin = float(min(column))
         columnMin = columnMin - columnMin/10
         columnMax = float(max(column))
         return np.linspace(columnMin, columnMax, numBins)

     df = pd.read_csv("iris.data", header = None, names = ["sepal_length", "sepal_width", "petal_length", "petal_width", "class"])
     names = ["very_small", "small", "medium", "large", "very-large"]
     nomdf = nominalize_dataframe(df, 6, names)
     # Get Columns Ratios
     for column in list(nomdf)[:-1]:
         for feature in set(nomdf[column]):
             print column, feature
             counts = nomdf[nomdf[column] == feature]["class"].value_counts()
             print calc_entropy(*list(counts / sum(counts))) * list(nomdf[column]).count(feature) / float(nomdf[column].count())
             #nomdf[nomdf[column] == feature][column].count() / float(nomdf[column].count())
    #+END_SRC

    #+RESULTS:
    #+begin_example
    sepal_length small
    0.211348729306
    sepal_length large
    0.244875419216
    sepal_length very_small
    0.0
    sepal_length medium
    0.39486511785
    sepal_length very-large
    0.0
    sepal_width 3.5
    0.0
    sepal_width 3.1
    0.124366813547
    sepal_width 2.0
    0.0
    sepal_width 3.0
    0.264547384391
    sepal_width 3.9
    0.0
    sepal_width 2.5
    0.0533333333333
    sepal_width 4.1
    0.0
    sepal_width 2.3
    0.0216340833189
    sepal_width 3.8
    0.0367318333622
    sepal_width 2.4
    0.0
    sepal_width 2.2
    0.0183659166811
    sepal_width 2.8
    0.0919546260299
    sepal_width 4.2
    0.0
    sepal_width 4.4
    0.0
    sepal_width 3.4
    0.0832681666378
    sepal_width 2.9
    0.0771186432965
    sepal_width 2.7
    0.0594645635903
    sepal_width 3.2
    0.134210319232
    sepal_width 3.7
    0.0
    sepal_width 3.6
    0.0183659166811
    sepal_width 2.6
    0.0323650198152
    sepal_width 3.3
    0.0583659166811
    sepal_width 4.0
    0.0
    petal_length 1.5
    0.0
    petal_length 1.0
    0.0
    petal_length 3.0
    0.0
    petal_length 4.0
    0.0
    petal_length 5.0
    0.0216340833189
    petal_length 3.5
    0.0
    petal_length 6.3
    0.0
    petal_length 4.1
    0.0
    petal_length 3.3
    0.0
    petal_length 4.6
    0.0
    petal_length 5.9
    0.0
    petal_length 1.9
    0.0
    petal_length 3.9
    0.0
    petal_length 6.0
    0.0
    petal_length 3.7
    0.0
    petal_length 5.2
    0.0
    petal_length 5.7
    0.0
    petal_length 4.3
    0.0
    petal_length 4.7
    0.0
    petal_length 6.9
    0.0
    petal_length 6.7
    0.0
    petal_length 4.8
    0.0266666666667
    petal_length 4.2
    0.0
    petal_length 4.9
    0.0323650198152
    petal_length 1.7
    0.0
    petal_length 1.2
    0.0
    petal_length 4.4
    0.0
    petal_length 5.4
    0.0
    petal_length 5.1
    0.0289901036373
    petal_length 6.4
    0.0
    petal_length 5.6
    0.0
    petal_length 1.4
    0.0
    petal_length 6.1
    0.0
    petal_length 1.3
    0.0
    petal_length 1.1
    0.0
    petal_length 1.6
    0.0
    petal_length 6.6
    0.0
    petal_length 3.6
    0.0
    petal_length 5.3
    0.0
    petal_length 3.8
    0.0
    petal_length 5.5
    0.0
    petal_length 5.8
    0.0
    petal_length 4.5
    0.0289901036373
    petal_width 0.5
    0.0
    petal_width 1.5
    0.0520017937319
    petal_width 2.0
    0.0
    petal_width 0.6
    0.0
    petal_width 2.5
    0.0
    petal_width 1.0
    0.0
    petal_width 1.1
    0.0
    petal_width 1.4
    0.0289901036373
    petal_width 1.9
    0.0
    petal_width 2.3
    0.0
    petal_width 2.4
    0.0
    petal_width 0.2
    0.0
    petal_width 1.8
    0.0331053480243
    petal_width 0.3
    0.0
    petal_width 1.2
    0.0
    petal_width 1.6
    0.0216340833189
    petal_width 1.3
    0.0
    petal_width 0.1
    0.0
    petal_width 0.4
    0.0
    petal_width 2.1
    0.0
    petal_width 1.7
    0.0133333333333
    petal_width 2.2
    0.0
#+end_example

* Week 04 - Neural Networks
** How do animals think?
   The brain is filled with neurons that fire when certain conditions are met.
** What's Hebb's Rule?
   If multiple neurons fire simultaneously the synaptic connection between them
   grows stronger.
** What's McCulloch and Pitts Neurons?
   Weighted inputs are put into an adder and then if the sum exceeds a threshold
   the neuron fires.
*** Is this realistic?
    - Not really, this is oversimplified. 
    - The main difference is that real neurons fire a spike train that encodes
      information, and not just an binary signal.
    - M&C can have negative weights, which has not been seen in real neurons.
*** So... neural networks. How do they classify things? How do they learn?
    - One word: pattern recognition.
      - Assuming that there is some pattern in the data, then by showing the
        neural network a few examples, we hope that it will find the pattern and
        predict the other examples correctly.
    - The learning of the neural network doesn't happen in the neurons
      themselves, rather it takes place between the neurons.
    - The only values we are allowed to change are the weights and threshold for
      firing.
** What is the Perceptron?
   - A simple neural network based off of M&P neurons.
   - /η/ defines the learning rate of the neural network.
* Python Thought of the Day
** Code
   #+BEGIN_SRC python :results output
  class myClass(object):
      def __init__(self, x):
          self._x = x

      @property
      def x(self):
          print "Calling the getter for my \"private\" member variable"
          return self._x

      @x.setter
      def x(self, x):
          print "Calling the setter for my \"private\" member variable"
          self._x = x

      @x.deleter
      def x(self):
          print "Calling the deleter for my \"private\" member variable"
          del self._x

  def example():
      classy = myClass(20)
      print(classy.x)
      classy.x = 30
      print(classy.x)
      del classy.x

  def main():
      print("Starting off the example")
      example()
      print("Ending the example")
  main()
   #+END_SRC

   #+RESULTS:
   : Starting off the example
   : Calling the getter for my "private" member variable
   : 20
   : Calling the setter for my "private" member variable
   : Calling the getter for my "private" member variable
   : 30
   : Calling the deleter for my "private" member variable
   : Ending the example
