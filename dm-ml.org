#+TITLE: Data Mining and Machine Learning
#+DATE: <2017-01-06 Fri>
#+AUTHOR: Colton Kopsa
#+EMAIL: Aghbac@Aghbac.local
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:t arch:headline
#+OPTIONS: author:t c:nil creator:comment d:(not "LOGBOOK") date:t
#+OPTIONS: e:t email:nil f:t inline:t num:t p:nil pri:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t toc:t todo:t |:t
#+CREATOR: Emacs 25.1.1 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export

* Week 01 - Introduction to Machine Learning
* Week 02 - k-Nearest Neighbors
** 01 PROVE : ASSIGNMENT - EXPERIMENT SHELL & HARDCODED CLASSIFIER
*** Submission
    When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

    Name: Colton Kopsa

    1. Please provide the URL of your public GitHub repository.

       https://github.com/coljamkop/k-nearest-neighbors

    For questions 2-6, please type "True" or "False" in front of the question number.

    2. My experiment shell can correctly load the Iris data-set:
       True
    3. My experiment shell randomizes the order of the instances (making sure to keep instances lined up with their appropriate targets) it and splits the data into a training set (70%) and a test set (30%)?
       True
    4. I have created a HardCoded classifier class with two methods: train and predict. The train method accepts training data (including targets). The predict method returns a prediction or classification for each instance it receives.
       True 
    5. The Experime  
    print calc_entropy(2/3.0, 1/3.0)
    nt Shell, processes the data, passes the training data to the classifier's€™s train method, the test data to the predict method, and then compares the predicted values against the correct answers, to produce an overall accuracy (on the test set).
    True
    6. I have run the HardCoded classifier on the Iris data-set and obtained a result.
       True
    7. What accuracy do you get when running the HardCoded classifier on the
       Iris data-set and why do you think that is? 
       - 33%, because the data-set is divided equally into 3 classes and my
         algorithm to predict is set to only predict one of the 3 classes, so
         it averages about 33%.




    8. Please select the category you feel best describes your assignment:
       1 - Some attempt was made
       2 - Developing, but significantly deficient
       3 - Slightly deficient, but still mostly adequate
       *4 - Meets requirements*
       5 - Shows creativity and excels above and beyond requirements

    1. Provide a brief justification (1-2 sentences) for selecting that
       category:
       1.  Although, I have yet to go above and beyond, my code fulfills
    all the requirements of the assignment

*** Submission 2
    When your assignment is complete, please answer the questions in this text file and upload it to I-Learn.

    1. Please provide the URL of your public GitHub repository.
       
    https://github.com/coljamkop/k-nearest-neighbors

    2. Briefly describe your overall approach to the task and highlight the most
       difficult part of this assignment.

    My overall approach was to get my classifier as generic as possible and then
    make up for it with data normalization. This most difficult thing was
    figuring out how to convert nominal data to numeric data in a clean fashion.
    In the end, I put everything in a set and then referenced the index of the
    set to numericise the data. This isn't great because it doesn't handle
    distances very well, but I ran out of time to do it any other way.

    3. Briefly describe how you handled the distance between nominal attributes.

       As of right now I just reference the index of the list representation of
       the set representation of the data. Had I had more time I was considering
       converting my nominal data to binary data, or trying to determine the
       bell curve of the data and use that to normalize the nominal data into
       numeric data.

    4. Briefly describe your process for handling numeric data on different scales (i.e., normalizing).

       Given more time, I would have scaled that data from 0 to 1 and weight based on a bell curve.

    5. Describe your results for the Iris data set. (For example, what level of accuracy did you see for different values of K? How did your implementation compare to existing implementations?)
       | k |  % |
       | 5 | 98 |
       | 3 | 96 |
       | 1 | 92 |

    6. Describe your results for the Car data set. (For example, what level of accuracy did you see for different values of K? How did your implementation compare to existing implementations?)
       
       Didn't finish

    7. Describe anything you did to go above and beyond the minimum standard requirements.


    8. Please select the category you feel best describes your assignment:
    A - Some attempt was made
    B - Developing, but signficantly deficient
    C - Slightly deficient, but still mostly adequate
    *D - Meets requirements*
    E - Shows creativity and excels above and beyond requirements


    9. Provide a brief justification (1-2 sentences) for selecting that category.
       
       Although I was unable to fully complete the assignment as specified, I
       think it meets the expectations discussed in class. Although I didn't do
       anything overly intense, I did find a lot of cool ways to make my code
       more succinct.
*** Code
    #+BEGIN_SRC python :tangle HardCodedClassifier.py
      class HardCodedClassifier(object):
          def __init__(self):
              pass
          def fit(self, inputVector, targetVector):
              pass
          def predict(self, inputVector):
              return [self.classify(x) for x in range(len(inputVector))]
          def classify(self, instance):
              return 0
    #+END_SRC
    
    #+BEGIN_SRC python :tangle KopsaClassifier.py
      import numpy
      class KopsaClassifier(object):
          def fit(self, inputvector, targetvector):
              self.inputvector = inputvector
              self.targetvector = targetvector
          def knn(self, instance, k):
              # find distance from instance for each element in inputvector
              distances = ((self.inputvector - instance)**2).sum(axis=1)
              # sort distances
              indices = numpy.argsort(distances, axis=0)
              nearestNeighbors = [self.targetvector[i] for i in indices[:k]]
              return nearestNeighbors
          def predict(self, inputvector):
              return [self.classify(x, 1) for x in inputvector]
          def classify(self, instance, k):
              nearestNeighbors = self.knn(instance, k)
              return max(set(nearestNeighbors), key=nearestNeighbors.count)
    #+END_SRC

    #+BEGIN_SRC python :tangle kNearestNeighbor.py :results output
      from sklearn import datasets
      from sklearn import model_selection
      from HardCodedClassifier import HardCodedClassifier
      from KopsaClassifier import KopsaClassifier


      def accuracy(output, target):
          truePositive = 0
          falsePositive = 0
          for i in range(len(output)):
              if output[i] == target[i]:
                  truePositive = truePositive + 1
              else:
                  falsePositive = falsePositive + 1
          return float(truePositive) / len(output)

      def run(inputVector, targetVector):
          # Shuffle input and target
          # knuth_shuffle(inputVector, targetVector)
          trainInput, testInput, trainTarget, testTarget = model_selection.train_test_split(inputVector,
                                                                            targetVector,
                                                                            test_size=0.33)
          classifier = KopsaClassifier()
          classifier.fit(trainInput, trainTarget)
          testOutput = classifier.predict(testInput)

          print accuracy(testOutput, testTarget)

      iris = datasets.load_iris()
      run(iris.data, iris.target)
    #+END_SRC

    #+BEGIN_SRC python :tangle irisDatasetImporter.py :results output
      import csv
      class IrisDataImporter(object):
         def __init__(self, filename):
            self.data = [row for row in csv.reader(open(filename, 'rb'))][:150]
            self.classes = list(set([row[4] for row in self.data]))
            self.targetVector = map(lambda x: self.classes.index(x), [row[4] for row in self.data])
            for x in range(0, len(data)):
               del self.data[x][len(data[x])-1]
               self.data[x] = map(float, self.data[x])

    #+END_SRC

* Week 03 - Decision Trees
** Prepare : Reading
*** Why use trees?
    - Computational costs of trees are low: /O/ ( log /N/ )
*** Information Theory
    - The mathematical study of quantifying, storing and transmitting information
    - 20 Questions is a good example where, when done correctly, over 500,000 
      animals can be represented by 20 bits (or 20 yes or no questions)
**** Entropy
     - A key to success in information theory is finding a feature that gives
       you the most information (has the highest entropy).
     - In the game of 20 questions, "Is it a cat?" has a lot lower entropy when
       compared to the question: "Is it a mammal?"
       - I think this is because the search space is greatly reduced with the
         higher entropy question/feature.
     - The key is to find features that split the data set as evenly as possible.
       - For example, if the feature is true/positive or false/negative in all
         examples, then the feature doesn't provide any additional information.
         - If all our examples are animals, then the feature "Has blood"
           provides no additional information along with the feature "is rock",
           unless this included a the Pokemon universe, in which case Geodude
           and his evolutions could be filtered out with that question.
     - When creating a decision tree, we use a greedy formula that looks for the
       highest entropy features as its nodes closest to the root.
     - Entropy can be calculated using the following formula:
     #+BEGIN_EXAMPLE
  def calc_entropy(p):
     if p!=0:
        return -p * np.log2(p)
     else:
        return 0
     #+END_EXAMPLE
** TODO Code
*** Building the tree:
    It would be cool to build the tree recursively, but have it return the
    entire built tree. If it were to take in all the things it needed, and then
    create the necessary children from it.

    #+BEGIN_SRC python :results output :tangle ID3_Tree.py
      import pandas as pd
      import numpy as np
      def calc_entropy(*probabilities):
          return sum([0 if p == 0 else -p * np.log2(p) for p in probabilities])
      def nominalize_dataframe(df, numBins, labels):
          tempdataframe = df.copy(True)
          for column in list(tempdataframe)[:-1]:
              tempdataframe[column] = nominalize_column(df[column], numBins, labels)
          return tempdataframe
      def nominalize_column(column, numBins, labels):
          if not isinstance(column, str):
              return pd.cut(column, create_bins(column, numBins), labels=labels)
      def create_bins(column, numBins):
          columnMin = float(min(column))
          columnMin = columnMin - columnMin/10
          columnMax = float(max(column))
          return np.linspace(columnMin, columnMax, numBins)
      def getColumnEntropy(df, column_name):
          column_entropy_sum = 0
          for feature in set(df[column_name]):
              column_entropy_sum += getFeatureEntropy(df, column_name, feature)
          return column_entropy_sum
      def getFeatureEntropy(df, column_name, feature):
          counts = df[df[column_name] == feature].ix[:, -1].value_counts()
          entropy = calc_entropy(*list(counts / sum(counts)))
          ratio = list(df[column_name]).count(feature) / float(df[column_name].count())
          return entropy * ratio
      def getBestColumn(df):
          return list(df)[np.argmin(map(lambda(x): getColumnEntropy(df, x), list(df)[:-1]))]
      def buildTree(df):
          best_column = getBestColumn(df)
          d_tree = dict()
          for feature in set(df[best_column]):
              if len(list(df)) == 2 or 0.0 == getFeatureEntropy(df, best_column, feature):
                  d_tree[best_column, feature] = list(df[df[best_column] == feature].ix[:, -1])[0]
              else:
                  d_tree[best_column, feature] = buildTree(df[df[best_column] == feature].drop([best_column], axis=1))
          return d_tree
      def classifyDataFrame(df, d_tree):
          return [classifyRow(row, d_tree) for index, row in df.iterrows()]
      def classifyRow(row, d_tree):
          current_column = d_tree.keys()[0][0]
          classification = d_tree.get((current_column, row[current_column]), d_tree[d_tree.keys()[0][0], d_tree.keys()[0][1]])
          if not isinstance(classification, dict):
              return classification
          else:
              return classifyRow(row, classification)
      def accuracy(output, target):
          truePositive = 0
          falsePositive = 0
          for i in range(len(output)):
              if output[i] == target[i]:
                  truePositive = truePositive + 1
              else:
                  falsePositive = falsePositive + 1
          return float(truePositive) / len(output)

      def displayDTree(tree):
          for key in tree.keys():
              if isinstance(tree[key], dict):
                  print "IF", key[0], "==", key[1], "AND"
                  displayDTree(tree[key])
              else:
                  print key[0], "==", key[1]
                  print "  ", tree[key]

      df0 = pd.read_csv("iris.data", header = None, names = ["sepal_length",
                                                            "sepal_width",
                                                            "petal_length",
                                                            "petal_width", "class"])
      names = ["very_very_small", "very_small", "small", "medium", "large", "very-large"]
      nomdf0 = nominalize_dataframe(df0, len(names)+1, names)
      # Get Columns Ratios
      d_tree0 = buildTree(nomdf0.sample(frac=.5))
      test_data0 = nomdf0.sample(frac=.2)
      output0 = classifyDataFrame(test_data0, d_tree0)
      target0 = list(test_data0["class"])
      print accuracy(output0, target0)

      df1 = pd.read_csv("lenses.data", sep='  ', names=["one", "two", "three", "four", "five"])
      d_tree1 = buildTree(df1.sample(frac=.2))
      test_data1 = df1.sample(frac=.8)
      output1 = classifyDataFrame(test_data1, d_tree1)
      target1 = list(test_data1.ix[:, -1])
      print accuracy(output1, target1)

      df2 = pd.read_csv("votes.data", names= ["Class Name",
                                              "handicapped-infants", "water-project-cost-sharing",
                                              "adoption-of-the-budget-resolution", "physician-fee-freeze", "el-salvador-aid",
                                              "religious-groups-in-schools", "anti-satellite-test-ban",
                                              "aid-to-nicaraguan-contras", "mx-missile", "immigration",
                                              "synfuels-corporation-cutback", "education-spending", "superfund-right-to-sue",
                                              "crime", "duty-free-exports", "export-administration-act-south-africa"])
      df2 = df2[list(df2)[::-1]]
      d_tree2 = buildTree(df2.sample(frac=.5))
      test_data2 = df2.sample(frac=.8)
      output2 = classifyDataFrame(test_data2, d_tree2)
      target2 = list(test_data2.ix[:, -1])
      print accuracy(output2, target2)
      displayDTree(d_tree2)
    #+END_SRC

* Week 04 - Neural Networks
** How do animals think?
   The brain is filled with neurons that fire when certain conditions are met.
** What's Hebb's Rule?
   If multiple neurons fire simultaneously the synaptic connection between them
   grows stronger.
** What's McCulloch and Pitts Neurons?
   Weighted inputs are put into an adder and then if the sum exceeds a threshold
   the neuron fires.
*** Is this realistic?
    - Not really, this is oversimplified. 
    - The main difference is that real neurons fire a spike train that encodes
      information, and not just an binary signal.
    - M&C can have negative weights, which has not been seen in real neurons.
*** So... neural networks. How do they classify things? How do they learn?
    - One word: pattern recognition.
      - Assuming that there is some pattern in the data, then by showing the
        neural network a few examples, we hope that it will find the pattern and
        predict the other examples correctly.
    - The learning of the neural network doesn't happen in the neurons
      themselves, rather it takes place between the neurons.
    - The only values we are allowed to change are the weights and threshold for
      firing.
** What is the Perceptron?
   - A simple neural network based off of M&P neurons.
   - /η/ defines the learning rate of the neural network.
* Python Thought of the Day
** Code
   #+BEGIN_SRC python :results output
  class myClass(object):
      def __init__(self, x):
          self._x = x

      @property
      def x(self):
          print "Calling the getter for my \"private\" member variable"
          return self._x

      @x.setter
      def x(self, x):
          print "Calling the setter for my \"private\" member variable"
          self._x = x

      @x.deleter
      def x(self):
          print "Calling the deleter for my \"private\" member variable"
          del self._x

  def example():
      classy = myClass(20)
      print(classy.x)
      classy.x = 30
      print(classy.x)
      del classy.x

  def main():
      print("Starting off the example")
      example()
      print("Ending the example")
  main()
   #+END_SRC

   #+RESULTS:
   : Starting off the example
   : Calling the getter for my "private" member variable
   : 20
   : Calling the setter for my "private" member variable
   : Calling the getter for my "private" member variable
   : 30
   : Calling the deleter for my "private" member variable
   : Ending the example
